#!/usr/bin/env python

"""
Usage: 
  psc_calc [options] --roi=<roi> --runs=<runs> --cond=<timing>... --out=<outdir>
  psc_calc [options] --roi=<roi> --runs=<runs> --events=<timing> --out=<outdir>

Extract percent signal change from a set of functional runs within an ROI. PSC
is extracted using A GLM with both finite impulse response (FIR) and FSL's
optimal FLOBS basis set.

Example:
  psc_calc --roi=IFG.shape.gii --runs=\"yo_01.func.gii yo_02.func.gii:yo_02-cfd1.txt:yo_02-cfd2.txt\" --cond=\"A:yo_01-A.txt yo_02-A.txt\" --cond=\"B:yo_01-B.txt yo_02-B.txt\" --out=BLAH_01-IFG
  psc_calc --roi=IFG.shape.gii --runs=\"yo_01.func.gii yo_02.func.gii:yo_02-cfd1.txt:yo_02-cfd2.txt\" --events=\"yo_01.csv yo_02.csv\" --out=BLAH_01-IFG

Arguments:
  --roi=<roi>        Binary mask image of ROI (.gii or .nii.gz)
  --runs=<runs>      List of functional runs in same space as ROI mask (.gii or
                     .nii.gz format). The list must be delimited by spaces and
                     enclosed in quotes. You may optionally specify one or more
                     confound text files per run. Each confound covariate should
                     correspond to a column in the confound text file, the same
                     length as the functional run. Confounds must be separated
                     from runs (and eachother) using colons. Confounds should
                     only be used to model out specific time points, and should
                     consist of only 1s and 0s.
  --cond=<timing>    List of timing files for a given condition, delimited by 
                     spaces and enclosed in quotes. The condition label must
                     precede the list, and be followed by a colon. There must
                     be a timing file for each run given, and the two lists
                     must be in corresponding order. This is a repeatable
                     argument; you should give a --cond argument for each
                     condition that you'd like to model. Timing files should be
                     in FSL's 3-column format.
  --events=<events>  List of combined timing files, one per run. The list should 
                     be delimited by spaces and enclosed in quotes. The first
                     column of each file should be the condition label,
                     followed by onset, duration, and height as in FSL's
                     3-column format. This argument is mutually exclusive
                     with --cond.
  --out=<outdir>     Output directory. Will be created if it doesn't exist, or
                     overwritten if it does.

Options:
  --X=<conds>        List of conditions to ignore. These conditions will be 
                     modeled but not reported. List should be delimited by
                     spaces and enclosed in double quotes (E.g. --X=\"A B C\").
  --resp=<conds>     List of conditions modeling behavioral responses. These 
                     conditions follow too closely to conditions of interest
                     to be easily disentangled. They will be modeled only in
                     the FLOBS basis function analysis (E.g. --resp=\"R\").
  --lag=<secs>       How long after the offset of a stimulus should the PSC
                     timecourse continue [default: 14].
  --tr=<secs>        TR in seconds [default: 2.0].
  --no-figs          Don't make figures, since they're time-consuming. 
  --log=<file>       Log file. Otherwise log information will be sent to
                     stdout/stderr.
  -h, --help         View this message.
"""

# So you can print to stderr/stdout
from __future__ import print_function

# Parse args before imports for fast help message
from docopt import docopt
ARGS = docopt(__doc__)

from datetime import datetime as dt
import nibabel as nib
from nibabel import gifti
import numpy as np
import os
import re
from shutil import copy, rmtree
import subprocess as subp
import sys
import tempfile as tf

log_file = None
log_dup = None
plt = None
PID = os.getpid()

def main():
  """
  Main function
  """
  # Define some global variables
  global log_file, outdir, log_dup, plt

  FLOBS = '{}/etc/default_flobs.flobs/hrfbasisfns.txt'.format(os.environ['FSLDIR'])
  try:
    FLOBS = np.genfromtxt(FLOBS)
  except IOError:
    raise RoiError('No FLOBS found in FSL dir ({}).'.format(os.environ['FSLDIR']))

  # ----------------------------------------------------------------------------
  # Read command line and check argument formats
  # ----------------------------------------------------------------------------

  # Read log option first, since it matters now
  if ARGS['--log'] is not None:
    if os.path.exists(os.path.dirname(os.path.abspath(ARGS['--log']))):
      log_file = open(ARGS['--log'], 'a')
    else:
      warning('Log file directory does not exist. Using stdout/stderr.')

  # ROI 
  roi = ARGS['--roi']
  
  # Funtionals and confounds
  funcs = ARGS['--runs']
  # Check for a likely format error
  if re.search(r'( :|: )', funcs) is not None:
    raise RoiError('--runs argument has bad format.')
  funcs = funcs.strip().split()
  funcs = [func.split(':') for func in funcs]
  # Sneaky way to unzip--thanks stack overflow
  funcs, func_cfds = zip(*[(func[0], func[1:]) for func in funcs])
  if len(funcs) == 0:
    raise RoiError('No functional runs provided')
  num_runs = len(funcs)

  # Timing files
  if ARGS['--cond'] != []: 
    tim_format=1
    tims = ARGS['--cond']
    tims = [re.split(r'\s*:?\s*', tim) for tim in tims]
    tims = {tim[0]: tim[1:] for tim in tims}
    for cond in tims:
      if len(tims[cond]) != num_runs:
        raise RoiError(('Number of timing files for condition {} ({}) ' +
                       'different from number ' +
                       'of runs ({}).').format(cond, len(tims[cond]), num_runs))
  else:
    tim_format=2
    tims = ARGS['--events']
    tims = tims.strip().split()

  # Exclude conditons
  Xconds = ARGS['--X']
  if Xconds is not None:
    Xconds = Xconds.strip().split()
  else:
    Xconds = []
  
  # resp conditons
  resp = ARGS['--resp']
  if resp is not None:
    resp = resp.strip().split()
  else:
    resp = []
  
  # TR
  tr = ARGS['--tr']
  try:
    tr = float(tr)
  except ValueError:
    raise RoiError('--tr option ({}) has bad format.'.format(tr))
  if tr < 0:
    raise RoiError('--tr option ({}) cannot be negative.'.format(tr))
  
  # PSC lag
  lag = ARGS['--lag']
  try:
    lag = float(lag)
  except ValueError:
    raise RoiError('--lag option ({}) has bad format.'.format(lag))
  if lag < 0:
    raise RoiError('--lag option ({}) cannot be negative.'.format(lag))
  # Round lag to nearest tr
  lag = int(np.round(lag/tr))

  # Make figures or not
  make_figs = not ARGS['--no-figs']
  if make_figs:
    # Importing pyplot can be time-consuming, so only do it if you have to
    # set Agg backend so that making figures doesn't depend on an X11 server
    # http://matplotlib.org/faq/howto_faq.html#matplotlib-in-a-web-application-server 
    import matplotlib as mpl
    mpl.use('Agg')
    from matplotlib import pyplot as plt
  
  # Output dir
  outdir = ARGS['--out']
  above_outdir = os.path.dirname(os.path.abspath(outdir))
  if not os.path.isdir(above_outdir):
    raise RoiError(('Directory above output ({}) ' +
                   'does not exist.').format(above_outdir))
  if os.path.isdir(outdir):
    warning('Output directory ({}) already exists. Overwriting.'.format(outdir))
  else:
    os.makedirs(outdir)
  log_dup = open('{}/log.txt'.format(outdir), 'w')

  # save command line and start time to log
  start_time = dt.now()
  start_time_s = start_time.strftime('%y-%m-%d %H:%M')
  log(('psc_calc job started at {}.\n' + 
      'Command line:\n    psc_calc {}').format(start_time_s, ' '.join(sys.argv[1:])))

  # ----------------------------------------------------------------------------
  # Preprocess functional data
  # ----------------------------------------------------------------------------
  
  log('Loading and preprocessing functional data.')
  # Read images as time x vertices/voxels Numpy arrays
  roi = img_read(roi)
  funcs = [img_read(func) for func in funcs]
  # Check that all images have same number of voxels/vertices
  if len(set([img.shape[1] for img in [roi] + funcs])) > 1:
    raise RoiError('Images provided have different numbers of voxels/vertices.')
  # Check ROI format
  if roi.shape[0] > 1:
    raise RoiError('ROI is a 4D image.')
  roi = roi[0, :]
  if set(np.unique(roi)) != {0, 1}:
    warning('ROI is not a binary image. Binarizing.')
    roi = (roi > 0).astype(int)
  # Calculate mask of voxels containing data for each run
  # Voxels without data should be constant
  data_masks = np.array([(np.var(func, axis=0) > np.finfo('f8').eps) for func in funcs])
  # Combine masks into one
  data_mask = np.product(data_masks, axis=0)
  log('Removing voxels/vertices that have near constant time series.')
  # combine with ROI mask
  mask = roi * data_mask
  log('{} voxels out of {} in ROI kept.'.format(np.sum(mask), np.sum(roi)))
  # mask time series'
  funcs = [func[:, mask==1] for func in funcs]
  # scale each voxel's time series to mean 1000
  tmeans = [np.reshape(np.mean(func, axis=0), (1, -1)) for func in funcs]
  funcs = [func * (1000.0/tmean) for func, tmean in zip(funcs, tmeans)]
  # Compute average time series for each run, and reshape as column vectors
  run_TSs = [np.reshape(np.mean(func, axis=1), (-1, 1)) for func in funcs]
  run_lens = [ts.size for ts in run_TSs]
  TS = np.vstack(run_TSs)
  # release some memory
  del funcs, tmeans, run_TSs, roi, mask, data_mask, data_masks
  
  # ----------------------------------------------------------------------------
  # Read confound files as Time x # confounds arrays
  # ----------------------------------------------------------------------------
  
  log('Reading confound files.')
  func_cfds = [read_run_cfds(run_cfds, run_len) 
               for run_cfds, run_len in zip(func_cfds, run_lens)]
  # Prepare confounds for linear regression
  # Arrange in block diagonal format
  cfd_mat = np.zeros((np.sum(run_lens), np.sum([cfd.shape[1] for cfd in func_cfds])))
  for i, run_cfd in enumerate(func_cfds):
    if run_cfd.shape[1] == 0:
      continue
    w_idx = int(np.sum([cfd.shape[1] for cfd in func_cfds[:i]]))
    l_idx = int(np.sum(run_lens[:i]))
    cfd_mat[l_idx: l_idx+run_lens[i], w_idx: w_idx+run_cfd.shape[1]] = run_cfd
  
  # ----------------------------------------------------------------------------
  # Read timing files
  # ----------------------------------------------------------------------------
  
  # Note: Height columns in timing files not used
  
  log('Reading timing files.')
  # Read timing files in first format
  # Cols: Onset, Duration, Height
  if tim_format == 1:
    tims = {cond: map(read_table, ctims) for cond, ctims in tims.iteritems()}
    conds = sorted(tims.keys())
    events = dict()
    for cond in conds:
      for i in range(num_runs):
        events[(i, cond)] = [l[:2] for l in tims[cond][i]]
  # Read timing files in second format
  # Cols: Cond, Onset, Duration, Height
  else:
    tims = [read_table(tim) for tim in tims]
    conds = [tuple(sorted(set([l[0] for l in tim]))) for tim in tims]
    # Check if each timing file has same conditions
    if len(set(conds)) != 1:
      warning("Not all runs have the same conditions")
    # Get sorted list of all conditions
    conds = sorted(set([c for run_conds in conds for c in run_conds]))
    # Make events dictionary: A list of onset, dur pairs for each run, condition
    events = {(i, cond): [] for i in range(num_runs) for cond in conds}
    for i in range(num_runs):
      for l in tims[i]:
        events[(i, l[0])].append(l[1:3])
  
  # Check conditions against exclude conditions
  if not set(Xconds) < set(conds):
    warning("Exclude conditions not a subset of conditions.")
    Xconds = sorted(set(Xconds) & set(conds))
  if not set(resp) < set(conds):
    raise RoiError("Response conditions not a subset of conditions.")
  # Check that each cond has some events
  event_count = {cond: np.sum([len(events[(i, cond)]) for i in range(num_runs)])
                 for cond in conds}
  for cond in conds:
    if event_count[cond] == 0:
      warning('No events for condition {}. Adding to excluded list.'.format(cond))
      # Actually, forget about that condition completely
      conds.remove(cond)
  mod_conds = sorted(set(conds) - set(resp))
  # Convert lists of onsets and durations to numpy arrays
  for k, ons_durs in events.iteritems():
    try:
      events[k] = np.array(ons_durs, dtype='f8')
    except ValueError:
      raise RoiError('Timing files should contain only numbers ' +
                     'in onset, duration columns.')
  
  # ----------------------------------------------------------------------------
  # Events preprocessing and bookkeeping (most of it to help with FIR model)
  # ----------------------------------------------------------------------------

  log('Events bookkeeping.')
  # Convert onsets and durations to trs
  # Record onsets and offsets relative to start of start of onset tr
  tr_ons, tr_durs, rel_ons, rel_offs = dict(), dict(), dict(), dict(), 
  for cond in mod_conds:
    for i in range(num_runs):
      ons_durs = events[(i, cond)]
      if ons_durs.size > 0:
        # floor because we want the index of the tr containing the onset
        tr_ons[(i, cond)] = (np.floor(ons_durs[:, 0]/tr)).astype('i4')
        # onsets and offsets relative to start of onset tr
        rel_ons[(i, cond)] = ons_durs[:, 0] - tr_ons[(i, cond)]*tr
        rel_offs[(i, cond)] = rel_ons[(i, cond)] + ons_durs[:, 1]
        # tr duration should be as such to include the tr containing the offset
        tr_durs[(i, cond)] = (np.ceil(rel_offs[(i, cond)]/tr)).astype('i4')
      else:
        tr_ons[(i, cond)] = []
        rel_ons[(i, cond)] = []
        rel_offs[(i, cond)] = []
        tr_durs[(i, cond)] = []
  # Combine relative onsets and offsets across runs
  rel_ons = {cond: np.hstack([rel_ons[(i, cond)] for i in range(num_runs)]) 
             for cond in mod_conds}
  for cond in mod_conds:
    if np.max(rel_ons[cond]) - np.min(rel_ons[cond]) > tr/2.0:
      warning(('Onsets for condition {} vary by more than 1/2 TR, ' +
              'relative to start of containing TR.').format(cond))
  # Combine relative offsets across runs
  rel_offs = {cond: np.hstack([rel_offs[(run, cond)] for run in range(num_runs)]) 
              for cond in mod_conds}
  # Collapse tr_durs to one number per condition (if possible) and add PSC lag
  tr_durs = [np.unique(np.hstack([tr_durs[(i, cond)] for i in range(num_runs)])) 
             for cond in mod_conds]
  for i, cond in enumerate(mod_conds):
    if np.size(tr_durs[i]) > 1:
      raise RoiError(('Not all events of condition {} have the same duration, ' + 
                     'even after rounding to the nearest TR.').format(cond))
    tr_durs[i] = tr_durs[i][0] + lag
  # Combine durations across runs
  all_durs = {cond: np.hstack([events[(i, cond)][:, 1] for i in range(num_runs) 
              if events[(i, cond)].size > 0]) for cond in conds}
  for cond in mod_conds:
    if np.max(all_durs[cond]) - np.min(all_durs[cond]) > tr/2.0:
      warning('Durations for condition {} vary by more than 1/2 TR'.format(cond))
  avg_durs = {cond: np.mean(all_durs[cond]) for cond in conds}
  # Decide predicted peak time points based on canonical HRF. Calculated by
  # computing max timepoint of convolved time series for durations 0.1s to
  # 20s. Max time point as a function of duraction is linear with slope =
  # 0.5375, intercept = 6.09625 until ~11 seconds, at which point constant
  # at ~11.75.
  peakt = dict()
  for cond in mod_conds:
    if avg_durs[cond] <= 11.0:
      peakt[cond] = 0.5375*avg_durs[cond] + 6.09625
    else:
      peakt[cond] = 11.75
    # peakt relative to start of first tr
    peakt[cond] += np.mean(rel_ons[cond])

  # ----------------------------------------------------------------------------
  # Calculate PSC timecourses and peaks using FIR model
  # ----------------------------------------------------------------------------
  
  log('Computing PSC using FIR model.')
  fir_bases = [np.eye(dur) for dur in tr_durs]
  # create FIR design matrix
  run_dms = []
  for i, run_len in enumerate(run_lens):
    # initialize design matrix: length of run + wiggle room x sum of condition
    # durations (in trs)
    dm = np.zeros((run_len+50, np.sum(tr_durs)))
    for j, cond in enumerate(mod_conds):
      dur = tr_durs[j]
      idx = np.sum(tr_durs[:j])
      for ons in tr_ons[(i, cond)]:
        dm[ons: ons+dur, idx: idx+dur] = fir_bases[j]
    # trim down to run length
    dm = dm[:run_len, :]
    run_dms.append(dm)
  DM = np.vstack(run_dms)
  bl, beta = calc_glm(TS, DM, 'fir', cfd_mat)
  # save FIR design mat
  fir_psc, fir_peak = dict(), dict()
  for j, cond in enumerate(mod_conds):
    dur = tr_durs[j]
    idx = np.sum(tr_durs[:j])
    # convert beta to psc
    fir_psc[cond] = (beta[idx: idx+dur]/bl)*100
    # decide weights for peak trs
    pt_frac = peakt[cond]/tr - np.floor(peakt[cond]/tr)
    weights = np.array([1. - pt_frac, 1., pt_frac])
    peak_tr = int(np.floor(peakt[cond]/tr))
    # weighted average of peak tr + tr before and after to get peak estimate
    fir_peak[cond] = np.sum(fir_psc[cond][peak_tr-1: peak_tr+2] * weights)/np.sum(weights)
  
  # ----------------------------------------------------------------------------
  # Calculate PSC HRF basis model
  # ----------------------------------------------------------------------------
 
  log('Computing PSC using FLOBS HRF basis model.')
  # make design matrix
  run_dms = []
  for i, run_len in enumerate(run_lens):
    # make high temporal resolution boxcar function for each condition
    # bases are in 0.05s resolution
    run_dur = int(np.ceil(run_len*tr*20))
    paradigm = np.zeros((run_dur, len(conds)))
    for j, cond in enumerate(conds):
      ons_durs = events[(i, cond)]
      for k in range(ons_durs.shape[0]):
        ons_idx = np.floor(ons_durs[k, 0]/0.05)
        off_idx = np.ceil(np.sum(ons_durs[k, :])/0.05)
        # should make it so that height is read from timing file
        paradigm[ons_idx:off_idx, j] = 1.0 
    # convolve with all three basis functions
    dm = [np.convolve(paradigm[:, j], FLOBS[:, k]) for j in range(len(conds))
          for k in range(3)]
    dm = np.hstack([covrt.reshape((-1, 1)) for covrt in dm])
    # trim to run length
    dm = dm[:run_dur, :]
    # subsample to functional resolution
    # take the prediction for the middle timepoint for each tr
    subsamp_tps = np.arange(tr/2., run_len*tr, tr)
    subsamp_inds = np.floor(subsamp_tps/0.05).astype('i4')
    run_dms.append(dm[subsamp_inds, :])
  DM = np.vstack(run_dms)
  bl, beta = calc_glm(TS, DM, 'hrf', cfd_mat)
  hrf_psc, psc_beta, hrf_len, stim_len = dict(), dict(), dict(), dict()
  for i, cond in enumerate(conds):
    cond_beta = beta[3*i: 3*(i+1)].reshape((-1, 1))
    # Round to nearest average duration + lag to nearest 0.5s, and find
    # containing .05s resolution index.
    hrf_len[cond], stim_len[cond] = [np.ceil(np.round(dur/0.5)*0.5/0.05)
                                     for dur in [avg_durs[cond] + lag*tr, avg_durs[cond]]]
    trial = np.zeros(hrf_len[cond])
    trial[:stim_len[cond]] = 1.0
    trial_hrf = [np.convolve(trial, FLOBS[:, j]).reshape((-1, 1)) for j in range(3)]
    trial_hrf = np.hstack(trial_hrf)[:hrf_len[cond], :]
    response = np.dot(trial_hrf, cond_beta)
    hrf_psc[cond] = (response/bl) * 100.0
    # use max of hrf bases as scalers to determine psc
    scalers = np.max(trial_hrf, axis=0)
    psc_beta[cond] = (scalers*cond_beta.reshape(-1)/bl) * 100.0
  
  # ----------------------------------------------------------------------------
  # Write data & save figures
  # ----------------------------------------------------------------------------
  
  log('Saving data and figures.')
  # save FIR data and figures
  incl_conds = sorted(set(mod_conds) - set(Xconds))
  fir_cols = 'Cond,N,Ons.min,Ons.max,Ons.mean,Off.min,Off.max,Off.mean,Peak.t,Peak.psc'.split(',')
  ps_len = np.max([tr_durs[i] for i, cond in enumerate(mod_conds) if cond in incl_conds])
  fir_cols += ['{0:0.1f}'.format(i*tr + tr/2.) for i in range(ps_len)]
  fir_data = np.ndarray((len(incl_conds)+1, len(fir_cols)), dtype=object)
  # default value
  fir_data[:, :] = np.nan
  fir_data[0, :] = fir_cols
  for i, cond in enumerate(incl_conds):
    fir_data[i+1, 0] = cond
    fir_data[i+1, 1] = event_count[cond]
    fir_data[i+1, 2:5] = np.min(rel_ons[cond]), np.max(rel_ons[cond]), np.mean(rel_ons[cond])
    fir_data[i+1, 5:8] = np.min(rel_offs[cond]), np.max(rel_offs[cond]), np.mean(rel_offs[cond])
    fir_data[i+1, 8:10] = peakt[cond], fir_peak[cond]
    fir_data[i+1, 10:10+fir_psc[cond].size] = fir_psc[cond]
  fir_data = fir_data.astype('S20')
  np.savetxt('{}/fir_results.csv'.format(outdir), fir_data, fmt='%.20s', delimiter=',')

  if make_figs:
    avg_stim_ons = np.mean([np.mean(rel_ons[cond]) for cond in incl_conds])
    avg_stim_off = np.mean([np.mean(rel_offs[cond]) for cond in incl_conds])
    avg_peakt = np.mean([peakt[cond] for cond in incl_conds])
    plot_hrf(fir_psc, incl_conds, '{}/fir_psc.pdf'.format(outdir), 
             [avg_stim_ons, avg_stim_off], [avg_peakt-tr, avg_peakt+tr], tr=tr, xunit=tr)

  # save hrf data and figures
  incl_conds = sorted(set(conds) - set(Xconds))
  hrf_cols = ['Cond', 'N', 'Stim.dur', 'Peak.psc', 'Delay.psc', 'Disp.psc']
  ps_len = int(np.max([hrf_len[cond] for cond in incl_conds]))
  hrf_cols += ['{0:0.2f}'.format(i*0.05) for i in range(ps_len)]
  hrf_data = np.ndarray((len(incl_conds)+1, len(hrf_cols)), dtype=object)
  # default value
  hrf_data[:, :] = np.nan
  hrf_data[0, :] = hrf_cols
  for i, cond in enumerate(incl_conds):
    hrf_data[i+1, 0] = cond
    hrf_data[i+1, 1] = event_count[cond]
    hrf_data[i+1, 2] = stim_len[cond]*0.05
    hrf_data[i+1, 3:6] = psc_beta[cond]
    hrf_data[i+1, 6:6+hrf_len[cond]] = hrf_psc[cond].reshape(-1)
  hrf_data = hrf_data.astype('S20')
  np.savetxt('{}/hrf_results.csv'.format(outdir), hrf_data, fmt='%.20s', delimiter=',')

  if make_figs:
    incl_conds = sorted(set(incl_conds) - set(resp))
    avg_stim_off = np.mean([stim_len[cond]*0.05 for cond in incl_conds])
    avg_peakt = np.mean([peakt[cond] for cond in incl_conds])
    plot_hrf(hrf_psc, incl_conds, '{}/hrf_psc.pdf'.format(outdir), [0, avg_stim_off],
             [avg_peakt-tr, avg_peakt+tr], tr=tr, xunit=0.05)

  # ----------------------------------------------------------------------------
  # Finish
  # ----------------------------------------------------------------------------
  
  fin_time = dt.now()
  fin_time_s = fin_time.strftime('%y-%m-%d %H:%M')
  run_time = str(fin_time - start_time)
  log('Finished psc_calc at {}!\nRun time: {}'.format(fin_time_s, run_time))

  return

class RoiError(Exception):
  """
  Class for script-specific errors.
  """
  def __init__(self, msg):
    if log_file is not None:
      out = log_file
    else:
      out = sys.stderr
    print('(pid={}) ERROR: {}'.format(PID, msg), file=out)
    if log_dup is not None:
      print('ERROR: {}'.format(msg), file=log_dup)
    Exception.__init__(self, msg)
    return

def warning(*objs):
  """
  Warning function.
  """
  if log_file is not None:
    out = log_file
  else:
    out = sys.stderr
  print('(pid={}) WARNING:'.format(PID), *objs, file=out)
  if log_dup is not None:
    print('WARNING:', *objs, file=log_dup)
  return

def log(*objs):
  """
  Logging function.
  """
  if log_file is not None:
    out = log_file
  else:
    out = sys.stdout
  print('(pid={})'.format(PID), *objs, file=out)
  if log_dup is not None:
    print(*objs, file=log_dup)
  return

def img_read(img):
  """
  Read a .nii, .nii.gz, or .gii image. Return a TxN numpy array.

  First dimension is time, second dimension contains all spatial dimensions
  flattened according to numpy.reshape function.
  """
  # don't be derailed by white space
  img = img.strip()
  if not os.path.isfile(img):
    raise RoiError('Image file ({}) does not exist.'.format(img))
  ext = r'(\.nii(\.gz)?|\.gii)$'
  img_ext = re.search(ext, img)
  if img_ext is None:
    raise RoiError(('Image file ({}) does not match any accepted' + 
                     'extensions (.nii, .nii.gz, .gii)').format(img))
  img_ext = img_ext.group()
  if img_ext == '.nii':
    try:
      img = nib.load(img).get_data()
    except:
      raise RoiError('Image file ({}) could not be loaded'.format(img))
    # reshape 4d nifti array to 2d, with time as first axis
    newshape = (-1, np.product(img.shape[:3]))
    img = img.reshape(newshape)
  elif img_ext == '.nii.gz':
    try:
      img = nib.load(img).get_data()
    except:
      # apparently some files (including our raw data) have a .gz extension but
      # are not gzipped, resulting in error. Remove extension before giving up.
      tmpdir = tf.mkdtemp(prefix='img_read-')
      img_copy = '{}/img.nii'.format(tmpdir)
      copy(img, img_copy)
      try:
        img = nib.load(img_copy).get_data()
        rmtree(tmpdir, ignore_errors=True)
      except:
        rmtree(tmpdir, ignore_errors=True)
        raise RoiError('Image file ({}) could not be loaded'.format(img))
    # reshape 4d nifti array to 2d, with time as first axis
    newshape = (-1, np.product(img.shape[:3]))
    img = img.reshape(newshape)
  else:
    try:
      img = gifti.read(img)
    except:
      raise RoiError('Image file ({}) could not be loaded'.format(img))
    img = np.array(map(lambda d: d.data, img.darrays))
  return img

def read_table(f, skip_head=False, comment="#", row_patt=r'[\r\n]+', col_patt=r'[ ,\t]+'):
  """
  General function for reading tabular data.
  """
  # don't be derailed by white space
  f = f.strip()
  try:
    table = open(f).read().strip()
  except IOError:
    raise RoiError('File ({}) does not exist.'.format(f))
  table = [row for row in re.split(row_patt, table) 
           if not (len(row) == 0 or row[0] == comment)]
  if skip_head:
    table = table[1:]
  table = [map(num_convert, re.split(col_patt, row.strip())) for row in table]
  return table

def num_convert(x):
  """
  Convert to number if possible.
  """
  try: return float(x)
  except: return x

def read_run_cfds(run_cfds, run_len):
  """
  Read a list standard FSL confound matrix file.
  # TRs X # Confound regressors.
  """
  if len(run_cfds) == 0:
    run_cfd = np.zeros((run_len, 0))
  else:
    run_cfds = [read_table(cfd) for cfd in run_cfds]
    try:
      run_cfds = [np.array(cfd, dtype='f8') for cfd in run_cfds]
    except ValueError:
      raise RoiError('Confound text file(s) have bad format')
    len_checks = [cfd.shape[0] == run_len for cfd in run_cfds]
    if False in len_checks:
      raise RoiError('Confound regressors should have same length ' +
                     'as matching functional run.')
    run_cfd = np.hstack(run_cfds)
  return run_cfd

def calc_glm(TS, DM, out_prefix, cfd_mat=None, intercept=True):
  """
  Calculate GLM beta weights. Return baseline and PSC betas.
  """
  # prepend intercept
  if intercept:
    DM = np.hstack([np.ones((DM.shape[0], 1)), DM])
  # append confound regressors
  if not (cfd_mat is None or cfd_mat.shape[1] == 0):
    DM = np.hstack([DM, cfd_mat])
  # save design matrix
  np.savetxt('{}/{}_design.txt'.format(outdir, out_prefix), DM, delimiter=' ')
  # calculate singular values
  try: 
    S = np.linalg.svd(DM, compute_uv=False)
  except LinAlgError:
    raise RoiError('Bad design matrix: SVD did not converge.')
  S_str = ' '.join(map(str, S))
  log('Singular values for {} design mat: {}.'.format(out_prefix, S_str))
  # determine if rank deficient
  # as in matrix_rank function
  eps = np.finfo('f8').eps
  thresh = S.max() * np.max(DM.shape) * eps
  rank = np.sum(S > thresh)
  log('Design matrix width for {}: {}; rank: {}.'.format(out_prefix, DM.shape[1], rank))
  if rank < S.size:
    raise RoiError('Bad design matrix: rank deficient.')
  # calculate betas
  # y = X*beta
  # beta = (X'X)^-1 * X' * y
  beta = np.dot(np.linalg.inv(np.dot(DM.T, DM)), np.dot(DM.T, TS))
  beta = beta.reshape(-1)
  if intercept:
    bl, beta = beta[0], beta[1:]
  else:
    bl = None
  return bl, beta

def plot_hrf(psc, conds, out, stim_on_off=None, peak_on_off=None, tr=2.0, xunit=2.0, figw=6.0, figh=4.0):
  """
  Plot hrf time course for a set of conditions, save to ``out``.
  
  Inputs:
    - ``psc``: dictionary mapping conditions to psc time courses.
    - ``conds``: conditions to plot.
    - ``out``: path to output figure.
    - ``stim_on_off``: tuple containing start and stop of stimulus.
    - ``peak_on_off``: tuple containing start and stop of peak window.
    - ``tr``: duration of tr, which will determine placement of xticks.
    - ``xunint``: unit of x axis (e.g. 2.0s, 0.05s).
    - ``figw``: width of figure.
    - ``figh``: height of figure.
  """

  leadbuff = 0.7
  backbuff = 0.25
  plotw = figw - (leadbuff + backbuff)

  bottombuff = 0.1
  topbuff = 0.2
  ploth = figh - (bottombuff + topbuff)

  f, ax = plt.subplots(1, 1, figsize=(figw, figh))

  pos = (leadbuff/figw, bottombuff/figh, plotw/figw, ploth/figh)
  ax.set_position(pos)
  ax.spines['right'].set_color('none')
  ax.spines['top'].set_color('none')
  ax.spines['bottom'].set_position(('data', 0))
  #only left and bottom ticks
  ax.yaxis.set_ticks_position('left')
  ax.xaxis.set_ticks_position('bottom')

  ymin = np.floor(np.min([np.min(psc[cond]) for cond in conds])/0.1)*0.1
  ymax = np.ceil(np.max([np.max(psc[cond]) for cond in conds])/0.1)*0.1
  ax.set_ylim(ymin, ymax)
  
  xmin = 0
  xmax = np.max([psc[cond].size for cond in conds])*xunit
  ax.set_xlim(xmin, xmax)
  xvals = np.arange(xunit/2., xmax, xunit)
  xticks = np.arange(tr, (np.floor(xmax/tr)+1)*tr, tr)
  xticklabels = map(lambda x: '{0:0.1f}'.format(x), xticks)
  plt.xticks(xticks, xticklabels)

  plt.xlabel('Seconds')
  plt.ylabel('Percent Signal Change')

  if stim_on_off is not None:
    ax.bar([stim_on_off[0]], [ymax], stim_on_off[1] - stim_on_off[0],
            color=(0.8, 0.8, 0.8, 0.5), edgecolor='none', label='stim')
  if peak_on_off is not None:
    ax.bar([peak_on_off[0]], [ymax], peak_on_off[1] - peak_on_off[0],
            color=(0.3, 0.3, 0.3, 0.5), edgecolor='none', label='peak')
  
  # don't show markers if the plot is 'high-res'
  if xvals.size > 50:
    ms = 0.
  else:
    ms = 7.
  for cond in conds:
    ax.plot(xvals[:psc[cond].size], psc[cond], marker='o', ms=ms,
            ls='-', lw=3., label=cond, clip_on=False)
  
  ax.legend(loc='upper right', bbox_to_anchor=(1.05, 1.05), borderpad=0.2,
            labelspacing=0.2)
  plt.savefig(out, dpi=400, transparent=True)
  return

if __name__ == '__main__':
  try:
    main()
  except RoiError as e:
    sys.exit(1)
  sys.exit(0)
