#!/usr/bin/env python

"""
Usage:
  roi_extract [options] --roi=<roi> --runs=<runs> --cond=<timing>... --out=<outdir>
  roi_extract [options] --roi=<roi> --runs=<runs> --events=<timing> --out=<outdir>

Extract percent signal change from a set of functional runs within an ROI. PSC
is extracted using either a finite-impulse response (FIR) model, FSL's flexible
HRF bases (FLOBS), or classic selective averaging.

References:
  Dale, A. M. (1999). Optimal experimental design for event-related fMRI. Human
    brain mapping, 8(2-3), 109-114.

  Ollinger, J. M., Shulman, G. L., & Corbetta, M. (2001). Separating processes
    within a trial in event-related functional MRI: I. The method.
    Neuroimage,13(1), 210-217.

Example:
  roi_extract --roi=IFG.shape.gii --runs=\"yo_01.func.gii yo_02.func.gii:yo_02-cfd1.txt:yo_02-cfd2.txt\" --cond=\"A:yo_01-A.txt yo_02-A.txt\" --cond=\"B:yo_01-B.txt yo_02-B.txt\" --out=BLAH_01-IFG
  roi_extract --roi=IFG.shape.gii --runs=\"yo_01.func.gii yo_02.func.gii:yo_02-cfd1.txt:yo_02-cfd2.txt\" --events=\"yo_01.csv yo_02.csv\" --out=BLAH_01-IFG

Arguments:
  --roi=<roi>        Binary mask image of ROI (.gii or .nii.gz)
  --runs=<runs>      List of functional runs in same space as ROI mask (.gii or
                     .nii.gz format). The list must be delimited by spaces and
                     enclosed in quotes. You may optionally specify one or more
                     confound text files per run. Each confound covariate should
                     correspond to a column in the confound text file, the same
                     length as the functional run. Confounds must be separated
                     from runs (and eachother) using colons. Confounds can be
                     discrete (e.g. a spike regressor) or continuous.
                     Continuous confounds will be centered.
  --cond=<timing>    List of timing files for a given condition, delimited by
                     spaces and enclosed in quotes. The condition label must
                     precede the list, and be followed by a colon. There must
                     be a timing file for each run given, and the two lists
                     must be in corresponding order. This is a repeatable
                     argument; you should give a --cond argument for each
                     condition that you'd like to model. Timing files should be
                     in FSL's 3-column format.
  --events=<events>  List of combined timing files, one per run. The list should
                     be delimited by spaces and enclosed in quotes. The first
                     column of each file should be the condition label,
                     followed by onset, duration, and height as in FSL's
                     3-column format. This argument is mutually exclusive
                     with --cond.
  --out=<outdir>     Output directory. Will be created if it doesn't exist, or
                     overwritten if it does.

Options:
  --mode=<mode>      Method for extracting PSC. Options are \"fir\", \"hrf\",
                     \"classic\", or combinations thereof. List should be
                     delimited by spaces and enclosed in double quotes
                     (E.g. --mode=\"fir hrf classic\") [default: fir].
  --X=<conds>        List of conditions to ignore. These conditions will be
                     modeled but not reported.  (E.g. --X=\"A B C\").
  --lag=<secs>       How long after the offset of a stimulus should the PSC
                     timecourse continue [default: 14.0].
  --peakw=<secs>     Width of the peak averaging window in seconds. The peak
                     window will be placed around the predicted peak time,
                     which is based on the shape of the canonical HRF.
                     Alternatively, you can specify two numbers to manually
                     define the onset and offset of the peak window
                     (E.g. --peakw=\"6.0 12.0\") [default: 6.0].
  --bl=<opts>        Baseline start, relative to stimulus offset, and baseline
                     stop, relative to stimulus onset, both in seconds. Only
                     relevant in \"classic\" mode [default: 14.0 0.0].
  --tr=<secs>        TR in seconds [default: 2.0].
  --no-figs          Don't make figures, since they can be time-consuming.
  --log=<file>       Log file. Otherwise log information will be sent to
                     stdout/stderr.
  -h, --help         View this message.
"""

# So you can print to stderr/stdout
from __future__ import print_function

# Parse args before imports for fast help message
from docopt import docopt
ARGS = docopt(__doc__)

from datetime import datetime as dt
import nibabel as nib
from nibabel import gifti
import numpy as np
import os
import re
from scipy.linalg import block_diag
import sys

from npdl_utils import Logger, NPDLError, img_read, read_table, init_table, normalize, resample, plot_hrf, fit_glm

logger = Logger(global_log=None, local_log=None, pid=os.getpid())

def main():
  """Main function"""

  # ----------------------------------------------------------------------------
  # Read command line and check argument formats
  # ----------------------------------------------------------------------------

  # Read log option first, since it matters now
  global logger
  global_log = ARGS['--log']
  if global_log is not None:
    logdir = os.path.dirname(os.path.abspath(global_log))
    if os.path.exists(logdir):
      logger.set_global_log(open(global_log, 'a'))
    else:
      logger.warning('Log file directory ({}) does not exist.'.format(logdir))

  # ROI
  roi = ARGS['--roi']

  # Funtionals and confounds
  funcs = ARGS['--runs']
  # Check for a likely format error
  if re.search(r'( :|: )', funcs) is not None:
    raise NPDLError('--runs argument has bad format.')
  funcs = funcs.strip().split()
  funcs = [func.split(':') for func in funcs]
  # Sneaky way to unzip--thanks stack overflow
  funcs, func_cfds = zip(*[(func[0], func[1:]) for func in funcs])
  if len(funcs) == 0:
    raise NPDLError('No functional runs provided')
  num_runs = len(funcs)

  # Timing files
  if ARGS['--cond'] != []:
    tim_format=1
    tims = ARGS['--cond']
    tims = [re.split(r'\s*:?\s*', tim) for tim in tims]
    tims = {tim[0]: tim[1:] for tim in tims}
    for cond in tims:
      if len(tims[cond]) != num_runs:
        raise NPDLError(('Number of timing files for condition {} ({}) ' +
                         'different from number ' +
                         'of runs ({}).').format(cond, len(tims[cond]), num_runs))
  else:
    tim_format=2
    tims = ARGS['--events']
    tims = tims.strip().split()

  # Extraction mode
  mode = set(ARGS['--mode'].split())
  if not (mode <= {'fir', 'hrf', 'classic'}):
    raise NPDLError('Invalid mode argument: {}.'.format(ARGS['--mode']))
  # Load FSL's optimal HRF basis functions
  if 'hrf' in mode:
    FLOBS = '{}/etc/default_flobs.flobs/hrfbasisfns.txt'.format(os.environ['FSLDIR'])
    try:
      FLOBS = np.genfromtxt(FLOBS)
    except IOError:
      raise NPDLError('No FLOBS found in FSL dir ({}).'.format(os.environ['FSLDIR']))

  # Exclude conditons
  Xconds = ARGS['--X']
  if Xconds is not None:
    Xconds = Xconds.strip().split()
  else:
    Xconds = []

  # TR
  tr = dur_check(ARGS['--tr'], '--tr')

  # PSC lag
  lag = dur_check(ARGS['--lag'], '--lag')
  # Round lag to nearest tr
  lag = int(np.round(lag/tr))

  # peak window
  peakw = ARGS['--peakw'].strip().split()
  if len(peakw) == 1:
    peakw = dur_check(peakw[0], '--peakw')
    man_peakw = False
  elif len(peakw) == 2:
    peakw = [dur_check(peakw[i], '--peakw') for i in 0, 1]
    man_peakw = True
  else:
    raise NPDLError('Bad --peakw format.')

  # Baseline options
  bl_opts = ARGS['--bl'].split()
  bl_opts = [dur_check(bl_opts[i], '--bl', nonneg=False) for i in 0, 1]
  default_bl = [14.0, 0.0]
  if 'classic' not in mode and bl_opts != default_bl:
    logger.warning(('Baseline parameters only apply in "classic" mode. ' +
                    'Ignoring user options.'))

  # Make figures or not
  make_figs = not ARGS['--no-figs']

  # Output dir
  outdir = ARGS['--out']
  above_outdir = os.path.dirname(os.path.abspath(outdir))
  if not os.path.isdir(above_outdir):
    raise NPDLError(('Directory above output ({}) ' +
                   'does not exist.').format(above_outdir))
  if os.path.isdir(outdir):
    logger.warning('Output directory ({}) already exists. Overwriting.'.format(outdir))
  else:
    os.makedirs(outdir)
  logger.set_local_log(open('{}/log.txt'.format(outdir), 'w'))

  # save command line and start time to log
  start_time = dt.now()
  start_time_s = start_time.strftime('%y-%m-%d %H:%M')
  logger.log(('roi_extract job started at {}.\n' +
      'Command line:\n    roi_extract {}').format(start_time_s, ' '.join(sys.argv[1:])))

  # ----------------------------------------------------------------------------
  # Preprocess functional data
  # ----------------------------------------------------------------------------

  logger.log('Loading and preprocessing functional data.')

  # Read images as time x vertices/voxels Numpy arrays
  roi = img_read(roi)
  funcs = [img_read(func) for func in funcs]

  # Check that all images have same number of voxels/vertices
  if len(set([img.shape[1] for img in [roi] + funcs])) > 1:
    raise NPDLError('Images provided have different numbers of voxels/vertices.')

  # Check ROI format
  if roi.shape[0] > 1:
    raise NPDLError('ROI is a 4D image.')
  roi = roi[0, :]
  if set(np.unique(roi)) != {0, 1}:
    logger.warning('ROI is not a binary image. Binarizing by thresholding at 0.')
    roi = (roi > 0).astype(int)

  # Calculate mask of voxels containing data for each run
  # Voxels without data should be constant
  logger.log('Removing voxels/vertices that have near constant time series.')
  data_masks = np.array([(np.var(func, axis=0) > np.finfo('f8').eps) for func in funcs])
  # Combine masks into one
  data_mask = np.product(data_masks, axis=0)
  # combine with ROI mask
  mask = roi * data_mask
  logger.log('{} voxels out of {} in ROI kept.'.format(np.sum(mask), np.sum(roi)))

  # mask time series'
  funcs = [func[:, mask==1] for func in funcs]

  # scale each voxel's time series to mean 1000
  tmeans = [np.reshape(np.mean(func, axis=0), (1, -1)) for func in funcs]
  funcs = [func * (1000.0/tmean) for func, tmean in zip(funcs, tmeans)]

  # Compute average time series for each run
  run_TSs = [np.mean(func, axis=1) for func in funcs]
  run_lens = [ts.size for ts in run_TSs]
  TS = np.hstack(run_TSs)
  # Save copy of time-series
  np.savetxt('{}/roi_ts.txt'.format(outdir), TS.reshape((-1, 1)))

  # release some memory
  del funcs, tmeans, run_TSs, roi, mask, data_mask, data_masks

  # ----------------------------------------------------------------------------
  # Read confound files as time X # confounds arrays
  # ----------------------------------------------------------------------------

  logger.log('Reading confound files.')
  func_cfds, spike_masks = zip(*[read_run_cfds(run_cfds, run_len)
                                for run_cfds, run_len in zip(func_cfds, run_lens)])

  # Prepare confounds for linear regression
  # Arrange in block diagonal format
  cfd_mat = block_diag(*func_cfds)

  # Concatenate spike masks
  spike_mask = np.hstack(spike_masks)

  # ----------------------------------------------------------------------------
  # Read and check timing files
  # ----------------------------------------------------------------------------

  logger.log('Reading and checking timing files.')

  # Form event table, cols: Run, Cond, Onset, Duration, Height
  # Read timing files in first format
  # One per cond, run; cols: Onset, Duration, Height
  event_table = []
  if tim_format == 1:
    conds = sorted(tims.keys())
    for cond in conds:
      for i in range(num_runs):
        tim = read_table(tims[cond][i])
        for event in tim:
          try:
            event_table.append([i, cond] + map(float, event))
          except ValueError:
            raise NPDLError(('Timing files must contain only numbers in onset,' +
                             'duration, height columns.'))

  # Read timing files in second format
  # One per run; cols: Cond, Onset, Duration, Height
  else:
    tims = [read_table(tim) for tim in tims]
    conds = [tuple(sorted(set([l[0] for l in tim]))) for tim in tims]
    # Check if each timing file has same conditions
    if len(set(conds)) != 1:
      logger.warning('Not all runs have the same conditions')
    # Get sorted list of all conditions
    conds = sorted(set([c for run_conds in conds for c in run_conds]))
    for i, tim in enumerate(tims):
      for event in tim:
        try:
          event_table.append([i, event[0]] + map(float, event[1:]))
        except ValueError:
          raise NPDLError(('Timing files must contain only numbers in onset,' +
                           'duration, height columns.'))

  # Construct events data structure
  events = Events(event_table, conds, run_lens, tr)

  # Warn about covariate centering
  cont_conds = []
  for cond in conds:
    if events.types[cond] == 'continuous':
      cont_conds.append(cond)
      logger.warning('Centering continuous predictor for condition {}.'.format(cond))
      if 'classic' in mode:
        logger.warning(('Continuous predictor for condition {} ' +
                        'will be ignored in classic mode.').format(cond))

  # Check conditions against exclude conditions
  if not set(Xconds) < set(conds):
    logger.warning('Exclude conditions not a subset of conditions.')
    Xconds = sorted(set(Xconds) & set(conds))

  # Check that each cond has some events
  for cond in conds:
    if events.counts[cond] == 0:
      logger.warning(('No events for condition: {}. ' +
                      'Pretending like it doesn\'t exist.').format(cond))
      conds.remove(cond)

  # Check how variable relative onsets are.
  avg_rel_ons = dict()
  for cond in conds:
    rel_ons = events.get_rel_onsets(cond=cond)
    if np.max(rel_ons) - np.min(rel_ons) > tr/2.:
      logger.warning(('Onsets for condition: {} vary by more than 1/2 TR, ' +
                      'relative to start of containing TR.').format(cond))
    avg_rel_ons[cond] = np.mean(rel_ons)

  # Check how variable tr durations are.
  tr_durs = dict()
  for cond in conds:
    tr_dur = np.unique(events.get_tr_durs(cond=cond))
    if tr_dur.size > 1 and not mode.isdisjoint({'fir', 'classic'}):
      raise NPDLError(('Not all events of condition: {} have the same duration, ' +
                       'even after rounding to the nearest TR.').format(cond))
    tr_durs[cond] = int(np.percentile(tr_dur, 50)) + lag

  # ----------------------------------------------------------------------------
  # Peak preparation
  # ----------------------------------------------------------------------------

  logger.log('Figuring out the peak windows.')

  peak_wins, avg_durs = dict(), dict()
  for cond in conds:
    avg_durs[cond] = np.mean(events.event_query(cond=cond)[:, 3])
    if man_peakw:
      peak_wins[cond] = PeakWindow(cond, avg_durs[cond], tr_durs[cond]*tr,
                                   man_peak_win=peakw, logger=logger)
    else:
      peak_wins[cond] = PeakWindow(cond, avg_durs[cond], tr_durs[cond]*tr,
                                   peak_win_dur=peakw, logger=logger)

  # ----------------------------------------------------------------------------
  # Save summary timing information
  # ----------------------------------------------------------------------------

  logger.log('Saving some information on event timing.')

  tim_info_header = ('Cond,N,Type,Rel.ons.min,Rel.ons.max,Rel.ons.mean,' +
                     'Rel.off.min,Rel.off.max,Rel.off.mean,TR.dur,' +
                     'Peak.win.start,Peak.win.stop').split(',')
  tim_info = init_table(tim_info_header, len(conds))

  for i, cond in enumerate(conds):
    N = events.counts[cond]
    tim_info[i+1, :3] = cond, N, events.types[cond]

    rel_ons = events.get_rel_onsets(cond=cond)
    rel_offs = events.get_rel_offsets(cond=cond)
    tim_info[i+1, 3:6] = [f(rel_ons) for f in [np.min, np.max, np.mean]]
    tim_info[i+1, 6:9] = [f(rel_offs) for f in [np.min, np.max, np.mean]]

    tim_info[i+1, 9] = tr_durs[cond]
    tim_info[i+1, 10:12] = peak_wins[cond].peak_win

  tim_info = tim_info.astype('S20')
  np.savetxt('{}/timing_info.csv'.format(outdir), tim_info, fmt='%.20s', delimiter=',')

  # ----------------------------------------------------------------------------
  # Calculate PSC timecourses and peaks
  # ----------------------------------------------------------------------------

  if 'fir' in mode:
    logger.log('Computing PSC using FIR model.')

    # Construct design mat
    fir_bases = [np.eye(tr_durs[cond]) for cond in conds]
    DM = events.build_design(conds, fir_bases, tr, boxcar=False)

    # Fit FIR model
    fir_bl, fir_beta = fit_glm(TS, DM, cfd_mat, outdir=outdir, out_prefix='fir',
                               logger=logger)

    # Organize results
    fir_psc, fir_peak = dict(), dict()
    for j, cond in enumerate(conds):
      dur = tr_durs[cond]
      idx = int(np.sum([tr_durs[c] for c in conds[:j]]))
      # convert beta to psc
      fir_psc[cond] = (fir_beta[idx: idx+dur]/fir_bl)*100
      # weighted average of peak window to get peak PSC estimate
      fir_peak[cond] = [peak_wins[cond].calc_peak(fir_psc[cond], tr, avg_rel_ons[cond])]

  if 'hrf' in mode:
    logger.log('Computing PSC using HRF model.')

    # Some constants
    hrf_bin_dur = .05
    ds_hrf_bin_dur = .25
    num_bases = 3

    # Construct design mat
    DM = events.build_design(conds, FLOBS, hrf_bin_dur, boxcar=True)

    # Fit HRF model
    hrf_bl, hrf_beta = fit_glm(TS, DM, cfd_mat, outdir=outdir, out_prefix='hrf',
                               logger=logger)

    # Organize results
    hrf_psc, hrf_peak = dict(), dict()
    for j, cond in enumerate(conds):
      cond_beta = hrf_beta[num_bases*j: num_bases*(j+1)].reshape((-1, 1))

      # length of psc response, and stimulus
      hrf_len = int(round(tr*tr_durs[cond]/hrf_bin_dur))
      stim_len = int(round(avg_durs[cond]/hrf_bin_dur))

      # Construct example trial (height of boxcar = 1.0)
      # NOTE: What should the height be for continuous predictors? Does it matter?
      # interpretation: PSC change per sd of change in predictor.
      trial = np.zeros(hrf_len)
      trial[:stim_len] = 1.0
      trial_hrf = [np.convolve(trial, FLOBS[:, k]).reshape((-1, 1))
                   for k in range(num_bases)]
      trial_hrf = np.hstack(trial_hrf)[:hrf_len, :]

      # Compute fitted HRF
      response = np.dot(trial_hrf, cond_beta).reshape(-1)
      # Downsample to 4 Hz
      response = resample(response, hrf_bin_dur, ds_hrf_bin_dur)
      hrf_psc[cond] = (response/hrf_bl) * 100.0

      # Peak esimation
      hrf_peak[cond] = [peak_wins[cond].calc_peak(hrf_psc[cond], ds_hrf_bin_dur, avg_rel_ons[cond])]

      # Add betas to peak psc list.
      # Use max of example trial hrf bases as scalers for transforming betas to psc.
      scalers = np.max(trial_hrf, axis=0)
      hrf_peak[cond].extend((scalers*cond_beta.reshape(-1)/hrf_bl) * 100.0)

  if 'classic' in mode:
    logger.log('Computing PSC using classic selective averaging.')

    # Filter continuous noise components from time-series
    cont_cfds = cfd_mat[:, spike_mask==0]
    if cont_cfds.shape[1] > 0:
      empty_DM = np.zeros((TS.shape[0], 0))
      cfd_bl, cfd_beta = fit_glm(TS, empty_DM, cfd_mat=cont_cfds)
      noise = np.dot(cont_cfds, cfd_beta.reshape((-1, 1)))
      filt_TS = TS - noise
    else:
      filt_TS = TS

    # Mark motion spikes in time-series
    spikes = np.sum(cfd_mat[:, spike_mask==1], axis=1)
    filt_TS[spikes > 0] = np.nan

    # Ignore any continuous predictors
    incl_conds = sorted(set(conds) - set(cont_conds))

    # Initialize trial-by-trial data table
    tbt_header = ['Run', 'Cond', 'Ons', 'Dur', 'Peak.psc']
    psc_len = np.max([tr_durs[cond] for cond in incl_conds])
    tbt_header += ['{0:0.1f}'.format(i*tr + tr/2.) for i in range(psc_len)]
    num_events = np.sum([events.counts[cond] for cond in incl_conds])
    tbt_data = init_table(tbt_header, num_events)

    # Initialize baseline mask
    bl_mask = np.ones(TS.size)

    idx = 1
    for i, run_len in enumerate(run_lens):
      tr_prefix = np.sum(run_lens[:i])
      for j, cond in enumerate(incl_conds):
        tr_dur = tr_durs[cond]
        cond_events = events.event_query(i, cond)[:, 2:4]
        # Loop through in chronological order
        cron_order = np.argsort(cond_events[:, 0])
        for k in cron_order:
          ons, dur = cond_events[k, :]

          # Remove event from baseline mask
          bl_stop = tr_prefix + int(np.floor((ons + bl_opts[1])/tr))
          bl_start = tr_prefix + int(np.ceil((ons + dur + bl_opts[0])/tr))
          bl_mask[bl_stop: bl_start] = 0.0

          # Update tbt data table
          tr_ons = tr_prefix + int(np.floor(ons/tr))
          # Make sure not to go past end of run, in case trial is clipped.
          tr_off = min(tr_ons + tr_dur, tr_prefix + run_len)
          bold = filt_TS[tr_ons: tr_off]
          peak = peak_wins[cond].calc_peak(bold, tr, avg_rel_ons[cond])
          tbt_data[idx, :5] = [i+1, cond, ons, dur, peak]
          tbt_data[idx, 5:5+bold.size] = bold
          idx += 1

    # Compute baseline and transform BOLD into psc
    bl = np.nanmean(filt_TS[bl_mask==1])
    tbt_data[1:, 4:] = 100. * (tbt_data[1:, 4:] - bl)/bl

    # Save trial by trial data
    tbt_data = tbt_data.astype('S20')
    np.savetxt('{}/classic_tbt_results.csv'.format(outdir),
               tbt_data, fmt='%.20s', delimiter=',')

    # Aggregate and organize results
    classic_psc, classic_peak = dict(), dict()
    for cond in incl_conds:
      cond_mask = tbt_data[:, 1] == cond
      agg = np.nanmean(tbt_data[cond_mask, 4:].astype(float), axis=0)
      classic_psc[cond] = agg[1:]
      classic_peak[cond] = [agg[0]]

  # ----------------------------------------------------------------------------
  # Write data & save figures
  # ----------------------------------------------------------------------------

  if 'fir' in mode:
    logger.log('Saving FIR mode PSC data.')
    incl_conds = sorted(set(conds) - set(Xconds))
    bin_dur = tr
    out_prefix = '{}/fir'.format(outdir)
    peak_colnames = ['Peak.psc']
    save_psc_data(fir_psc, fir_peak, events.counts, avg_rel_ons, avg_durs,
                  peak_wins, incl_conds, bin_dur, tr, out_prefix,
                  peak_colnames=peak_colnames)

  if 'hrf' in mode:
    logger.log('Saving HRF mode PSC data.')
    incl_conds = sorted(set(conds) - set(Xconds))
    bin_dur = ds_hrf_bin_dur
    out_prefix = '{}/hrf'.format(outdir)
    peak_colnames = ['Peak.psc', 'Beta.amp', 'Beta.delay', 'Beta.disp']
    save_psc_data(hrf_psc, hrf_peak, events.counts, None, avg_durs,
                  peak_wins, incl_conds, bin_dur, tr, out_prefix,
                  peak_colnames=peak_colnames)

  if 'classic' in mode:
    logger.log('Saving Classic mode PSC data.')
    incl_conds = sorted(set(conds) - set(Xconds).union(set(cont_conds)))
    bin_dur = tr
    out_prefix = '{}/classic'.format(outdir)
    peak_colnames = ['Peak.psc']
    save_psc_data(classic_psc, classic_peak, events.counts, avg_rel_ons, avg_durs,
                  peak_wins, incl_conds, bin_dur, tr, out_prefix,
                  peak_colnames=peak_colnames)

  # ----------------------------------------------------------------------------
  # Finish
  # ----------------------------------------------------------------------------

  fin_time = dt.now()
  fin_time_s = fin_time.strftime('%y-%m-%d %H:%M')
  run_time = str(fin_time - start_time)
  logger.log('Finished roi_extract at {}! Run time: {}'.format(fin_time_s, run_time))
  return

def dur_check(dur, optname, nonneg=True):
  """Check if command-line option is numeric and non-negative."""
  try:
    dur = float(dur)
  except ValueError:
    raise NPDLError('{} option ({}) has bad format.'.format(optname, dur))
  if dur < 0 and nonneg:
    raise NPDLError('{} option ({}) cannot be negative.'.format(optname, dur))
  return dur

def read_run_cfds(run_cfds, run_len):
  """Read a list of standard FSL confound matrix files.

  Returns:
    run_cfd (2D Array): First axis is time, second is the individual confounds.
    spike_mask (1D Array): Size is number of confounds. Indicates whether
      confound is a spike regressor.
  """
  if len(run_cfds) == 0:
    run_cfd = np.zeros((run_len, 0))
    spike_mask = np.zeros(0, dtype=int)

  else:
    try:
      run_cfds = [np.genfromtxt(cfd, dtype='f8') for cfd in run_cfds]
    except:
      raise NPDLError('Confound text file(s) have bad format')

    len_checks = [cfd.shape[0] == run_len for cfd in run_cfds]
    if False in len_checks:
      raise NPDLError('Confound regressors should have same length ' +
                     'as matching functional run.')

    # Need to reshape in case you get a 1-D confound
    run_cfds = [cfd.reshape((run_len, -1)) for cfd in run_cfds]
    run_cfd = np.hstack(run_cfds)

    # Determine type of confound
    # Normalize continuous covariates (in particular, center)
    spike_mask = np.ones(run_cfd.shape[1], dtype=int)
    for i in range(run_cfd.shape[1]):
      if np.sum(run_cfd[:, i] == np.round(run_cfd[:, i])) < run_len:
        run_cfd[:, i] = normalize(run_cfd[:, i])
        spike_mask[i] = 0
      elif np.sum(run_cfd[:, i]) != 1.:
        raise NPDLError(('Only continuous covariates and spike regressors' +
                        'accepted as confounds.'))
  return run_cfd, spike_mask

class Events(object):
  """Trial event sequences.

  Args:
    event_table (2D array, dtype=object): Each row is an event containing: run,
      condition, onset, duration, height.
    conds (list): List of condition names in event_table.
    run_lens (list): List of run lengths.
    tr (float): TR of functional data in seconds.
  """

  def __init__(self, event_table, conds, run_lens, tr):
    self.event_table = np.array(event_table, dtype=object)
    self.conds = conds
    self.run_lens = run_lens
    self.tr = tr
    self.counts = {cond: self.event_query(cond=cond).shape[0] for cond in self.conds}
    self.types = {cond: self.check_event_type(cond) for cond in self.conds}
    return

  def event_query(self, run=None, cond=None, ons=None):
    """Look up events matching some criteria."""
    query = self.event_table[:, :]
    for i, key in enumerate([run, cond, ons]):
      if key is not None:
        query = query[(query[:, i] == key), :]
    return query

  def check_event_type(self, cond, center_cont=True, norm_cont=True):
    """Check whether event is a dummy or continuous, and optionally center."""
    cond_mask = (self.event_table[:, 1] == cond)
    heights = self.event_table[cond_mask, 4]
    if set(np.unique(heights)) <= {1., 0.}:
      event_type = 'dummy'
    else:
      event_type = 'continuous'
      if center_cont:
        heights = heights - np.mean(heights)
      if norm_cont:
        heights = heights/np.std(heights)
      self.event_table[cond_mask, 4] = heights
    return event_type

  def build_design(self, incl_conds, bases, bin_dur, boxcar=True):
    """Construct design matrix by convolving event sequence with bases.

    Args:
      incl_conds (list): List of condition names to include.
      bases (2D array or list): Hemodynamic basis functions. Can be either
        a single set (2D array, shape (time, num_bases)), or a different set
        per condition (list of 2D arrays).
      bin_dur (float): Temporal resolution of basis functions.
      boxcar (bool): Whether events whould be modeled as boxcars or delta
        impulses.

    Returns:
      dm: Convolved design matrix (2D array, shape (time, num_regressors))
    """
    # If only one set of bases given, expand to a list for compatibility
    if isinstance(bases, np.ndarray):
      bases = [bases]*len(incl_conds)

    run_dms = []
    for i, run_len in enumerate(self.run_lens):
      run_len_bins = int(round(run_len*self.tr/bin_dur))
      paradigm = np.zeros((run_len_bins, len(incl_conds)))
      for j, cond in enumerate(incl_conds):
        events = self.event_query(i, cond)
        for k in range(events.shape[0]):
          ons, dur, ht = events[k, 2:]
          ons_ind = int(np.floor(ons/bin_dur))
          if boxcar:
            off_ind = int(np.ceil((ons + dur)/bin_dur))
          else:
            off_ind = ons_ind + 1
          paradigm[ons_ind:off_ind, j] = ht

      # Convolve with bases.
      run_dm = [np.convolve(paradigm[:, j], bases[j][:, k])[:run_len_bins]
                for j in range(len(incl_conds))
                for k in range(bases[j].shape[1])]
      # Downsample for basis to TR resolution
      run_dm = [resample(covrt, bin_dur, self.tr) for covrt in run_dm]
      # Form design matrix
      run_dm = np.hstack([covrt.reshape((-1, 1)) for covrt in run_dm])
      run_dms.append(run_dm)

    dm = np.vstack(run_dms)
    return dm

  def get_tr_onsets(self, run=None, cond=None):
    onsets = self.event_query(run, cond)[:, 2].astype(float)
    tr_onsets = np.floor(onsets/self.tr).astype(int)
    return tr_onsets

  def get_rel_onsets(self, run=None, cond=None):
    onsets = self.event_query(run, cond)[:, 2].astype(float)
    rel_onsets = onsets - self.get_tr_onsets(run, cond)*self.tr
    return rel_onsets

  def get_rel_offsets(self, run=None, cond=None):
    durs = self.event_query(run, cond)[:, 3].astype(float)
    rel_offsets = self.get_rel_onsets(run, cond) + durs
    return rel_offsets

  def get_tr_durs(self, run=None, cond=None):
    tr_durs = np.ceil(self.get_rel_offsets(run, cond)/self.tr).astype(int)
    return tr_durs

class PeakWindow(object):
  """PSC peak window.

  Args:
    cond (str): Name of condition.
    trial_dur (float): Length of trial in seconds.
    psc_dur (float): Length of entire PSC response in seconds.
    peak_win_dur (float): Length of peak averaging window in seconds.
    man_peak_win (tuple): 2-tuple of floats representing start and end of
      manual peak averaging window, in seconds, relative to trial onset.
    logger (Logger instance): For issuing command-line warnings to user.
  """

  def __init__(self, cond, trial_dur, psc_dur, peak_win_dur=6.0,
               man_peak_win=None, logger=None):
    self.cond = cond
    self.trial_dur = trial_dur
    self.peakt = self.calc_peakt(trial_dur)
    self.psc_dur = psc_dur
    if man_peak_win is None:
      self.man_peak = False
      self.peak_win = (self.peakt - peak_win_dur/2.,
                       self.peakt + peak_win_dur/2.)
    else:
      self.man_peak = True
      self.peak_win = man_peak_win
    self.logger = logger
    self.check_peak_win()
    return

  def calc_peakt(self, trial_dur):
    """Decide predicted peak time based on canonical HRF.

    Calculated by computing max timepoint of convolved time series for
    durations 0.1s to 20s. Max time point as a function of duration is linear
    with slope = 0.5375, intercept = 6.09625 until ~11 seconds, at which point
    constant at ~11.75.

    Note: This isn't quite right. peak time for 11 is 12.009. Need to revisit.

    Args:
      trial_dur (float): Duration of trial in seconds.

    Returns:
      peakt (float): Time in seconds of predicted peak.
    """
    if trial_dur <= 11.0:
      peakt = 0.5375*trial_dur + 6.09625
    else:
      peakt = 11.75
    return peakt

  def check_peak_win(self):
    """Check that peak window fits in trial HRF."""
    if self.peak_win[0] < 0.0:
      self.peak_win[0] = 0.0
      if self.logger is not None:
        self.logger.warning(('Start of peak window < 0 sec for cond: {}. ' +
                             'Setting to 0.').format(self.cond))
    if self.peak_win[1] > self.psc_dur:
      self.peak_win[1] = self.psc_dur
      if self.logger is not None:
        logger.warning(('End of peak window is longer than trial HRF ' +
                        'for cond: {}. Truncating.').format(self.cond))
    return

  def calc_peak(self, hrf, bin_dur, rel_onset=0.0):
    """Calculate peak signal of HRF within peak window.

    Args:
      hrf (1D array): 1D time-series to analyze.
      bin_dur (float): Duration of time-series bin in seconds.
      rel_onset (float): Onset of trial in seconds relative to start of first
        bin.

    Returns:
      peak (float): Weighted average of hrf from peak window.
    """
    hrf_dur = hrf.size * bin_dur

    # Shift peak win depending on relative onset
    peak_win = np.array(self.peak_win) + rel_onset

    # Determine weights based on peak window.
    # Weight on bin b is the fraction of b within peak window.
    # NOTE: IF peak start and stop are within 1 TR, weights aren't sensible.
    bin_onsets = np.arange(hrf.size) * bin_dur
    bin_offsets = bin_onsets + bin_dur
    frac_after_peak_start = np.clip((bin_offsets - peak_win[0])/bin_dur, 0., 1.)
    frac_before_peak_stop = np.clip((peak_win[1] - bin_onsets)/bin_dur, 0., 1.)
    weights = np.min(np.vstack([frac_after_peak_start, frac_before_peak_stop]), axis=0)

    # Mask nans in hrf.
    nanmask = np.isnan(hrf).astype(int)
    hrf = hrf[nanmask==0]
    weights = weights[nanmask==0]

    # Compute peak.
    if hrf.size == 0:
      peak = np.nan
    else:
      peak = np.sum(hrf*weights)/np.sum(weights)
    return peak

def save_psc_data(psc, peak, counts, rel_ons, durs, peak_wins, incl_conds,
                  bin_dur, tr, out_prefix, make_figs=True, peak_colnames=None):
  """Save PSC peaks and time-courses.

  Args:
    psc (dict): Dictionary mapping conditions to PSC timecourses (1D arrays).
    peak (dict): Dictionary mapping conditions to peak PSC values (lists or 1D
      arrays).
    counts (dict): Dictionary mapping conditions to event counts.
    rel_ons (dict or None): Dictionary mapping conditions to onsets relative to
      start of TR (or None, in which case 0 is assumed)
    durs (dict): Dictionary mapping conditions to event durations.
    peak_wins (dict): Dictionary mapping conditions to PeakWindow instances.
    incl_conds (list): List of conditions to include.
    bin_dur (float): Temporal resolution of PSC time-courses.
    tr (float): Temporal resolution of original fMRI data (affects figure
      x-axis only.)
    out_prefix (str): Prefix path for output files.
    make_figs (bool): Whether to make HRF figure.
    peak_colnames (list): List of column names for PSC peak values.
  """
  # Put data table header together.
  header = ['Cond', 'N']
  if peak_colnames is not None:
    header.extend(peak_colnames)
  else:
    peak_len = np.max([len(peak[cond]) for cond in incl_conds])
    header.extend(['Peak.psc.{}'.format(i+1) for i in range(peak_len)])
  psc_len = np.max([psc[cond].size for cond in incl_conds])
  header.extend(['{0:0.2f}'.format(i*bin_dur + bin_dur/2.) for i in range(psc_len)])

  # Initialize data table.
  data_table = init_table(header, len(incl_conds))

  # Fill in data.
  for i, cond in enumerate(incl_conds):
    data_table[i+1, :2] = [cond, counts[cond]]
    peak_stop = 2+len(peak[cond])
    psc_stop = peak_stop + psc[cond].size
    data_table[i+1, 2:peak_stop] = peak[cond]
    data_table[i+1, peak_stop:psc_stop] = psc[cond]
  data_table = data_table.astype('S20')
  np.savetxt('{}_results.csv'.format(out_prefix), data_table, fmt='%.20s', delimiter=',')

  if rel_ons is None:
    rel_ons = {cond: 0.0 for cond in incl_conds}
  # Save figures.
  if make_figs:
    avg_stim_ons = np.mean([rel_ons[cond] for cond in incl_conds])
    avg_stim_off = np.mean([rel_ons[cond] + durs[cond] for cond in incl_conds])
    avg_peak_win = [np.mean([peak_wins[cond].peak_win[i] for cond in incl_conds])
                    for i in [0, 1]]
    plot_hrf(psc, incl_conds, '{}_psc.pdf'.format(out_prefix),
             None, [avg_stim_ons, avg_stim_off], avg_peak_win, tr=tr, xunit=bin_dur)
  return

if __name__ == '__main__':
  try:
    main()
  except NPDLError as e:
    logger.error(e)
    sys.exit(1)
  sys.exit(0)
